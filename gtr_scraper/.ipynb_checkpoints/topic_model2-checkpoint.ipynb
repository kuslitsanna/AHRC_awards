{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7bcdb2-81d7-4ac7-b829-a15926515d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54cf278-74bc-453e-8acc-d71b0e34ab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [india, event, mark, conclusion, funded, pilot...\n",
      "1    [action, based, brain, provocation, philosophy...\n",
      "2    [imprint, forensic, historical, investigation,...\n",
      "3    [structural, typological, variation, dialect, ...\n",
      "4    [judging, image, making, management, consumpti...\n",
      "Name: AllText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('filtered_tokenised_abstracts.pkl', 'rb') as file:\n",
    "    filtered_tokenised_abstracts = pickle.load(file)\n",
    "\n",
    "print(filtered_tokenised_abstracts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e203bfb-2aad-46f7-bf4b-d872e99d6bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30035"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary(filtered_tokenised_abstracts)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80426bc8-4158-4d2c-a1b5-b5b8777cfbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Document-Term Matrix (DTM) using the dictionary\n",
    "dtm = [dictionary.doc2bow(doc) for doc in filtered_tokenised_abstracts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef642d-2218-44a9-989e-a302dd8e5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_priors(eta, topic, words, p=.8):\n",
    "    word_indexes = [word2id[w] for w in words]\n",
    "    eta[topic, word_indexes] *=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77625373-c365-4dc9-80d1-f9dd3e97b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "eta = np.full((n, len(dictionary)), 1/(len(dictionary)*n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb24537-0203-4f5d-b0cb-b7a3aec1779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your predefined keywords for each topic\n",
    "digital_keywords = [\"technology\", \"data\", \"digital\", \"internet\", \"information\", \"digitise\"]\n",
    "urban_keywords = [\"urban\", \"city\", \"planning\", \"space\", \"access\", \"culture\", \"design\", \"civic\", \"transport\"]\n",
    "local_keywords = [\"local\", \"region\", \"regional\", \"heritage\", \"history\", \"identity\", \"site\"]\n",
    "health_keywords = [\"health\", \"wellbeing\", \"mental_health\", \"care\", \"social_care\", \"disability\", \"inclusion\", \"access\", \"community\", \"support\"]\n",
    "healthcare_keywords = [\"hospital\", \"nhs\", \"disease\", \"infection\", \"death\", \"anti_microbial\", \"pandemic\", \"virus\"]\n",
    "cooperation_keywords = [\"network\", \"cooperation\", \"partnership\", \"partner\", \"collaborate\", \"collaboration\", \"impact\"]\n",
    "international_keywords = [\"international\", \"global\", \"transnational\", \"world_wide\", \"world\"]\n",
    "race_keywords = [\"race\", \"colonial\", \"slavery\", \"slave_trade\", \"black\", \"ethnic\", \"african\", \"indian\", \"empire\", \"indigenous\", \"native\"]\n",
    "justice_keywords = [\"justice\", \"equality\", \"diversity\", \"social\", \"reparative\", \"reparation\", \"repatriation\"]\n",
    "conflict_keywords = [\"conflict\", \"violence\", \"genocide\", \"terrorism\", \"war\", \"displacement\"]\n",
    "learning_keywords = [\"resource\", \"learning\", \"teaching\", \"school\", \"activity\", \"youth\", \"education\", \"student\", \"teacher\", \"classroom\"]\n",
    "public_keywords = [\"engagement\", \"public\", \"engage\", \"audience\", \"participation\", \"participant\", \"community\", \"experience\", \"visitor\"]\n",
    "climate_keywords = [\"climate\", \"climate_change\", \"environment\", \"nature\", \"natural\", \"animal\", \"plant\", \"resource\", \"waste\", \"future\", \"food\", \"science\",\"global_warming\", \"argiculture\", \"risk\",\"environmental\", \"emergency\"]\n",
    "religion_keywords = [\"religion\", \"faith\", \"church\", \"belief\", \"value\", \"worldview\"]\n",
    "migration_keywords = [\"migration\", \"migrant\", \"refugee\"]\n",
    "\n",
    "seed_topics = [digital_keywords, urban_keywords, local_keywords, health_keywords, healthcare_keywords, cooperation_keywords, \n",
    "               international_keywords, race_keywords, justice_keywords, conflict_keywords, learning_keywords, public_keywords,\n",
    "               climate_keywords, religion_keywords, migration_keywords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e7cfd3f-1a72-49d8-80ac-1d21fad5c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.108*\"community\" + 0.028*\"local\" + 0.015*\"partner\" + 0.014*\"group\" + 0.013*\"city\" + 0.013*\"organisation\" + 0.010*\"engagement\" + 0.010*\"working\" + 0.009*\"urban\" + 0.008*\"practice\"\n",
      "Topic 1: 0.023*\"science\" + 0.013*\"nature\" + 0.012*\"question\" + 0.010*\"idea\" + 0.010*\"humanity\" + 0.009*\"public\" + 0.009*\"field\" + 0.009*\"communication\" + 0.008*\"form\" + 0.008*\"researcher\"\n",
      "Topic 2: 0.030*\"music\" + 0.028*\"digital\" + 0.018*\"film\" + 0.016*\"performance\" + 0.012*\"medium\" + 0.012*\"technology\" + 0.011*\"industry\" + 0.010*\"form\" + 0.010*\"audience\" + 0.008*\"sound\"\n",
      "Topic 3: 0.020*\"public\" + 0.014*\"future\" + 0.012*\"child\" + 0.011*\"social\" + 0.011*\"human\" + 0.009*\"practice\" + 0.009*\"family\" + 0.008*\"space\" + 0.008*\"issue\" + 0.008*\"environment\"\n",
      "Topic 4: 0.020*\"international\" + 0.015*\"political\" + 0.012*\"case\" + 0.012*\"religious\" + 0.011*\"context\" + 0.011*\"social\" + 0.011*\"state\" + 0.010*\"finding\" + 0.010*\"academic\" + 0.009*\"concept\"\n",
      "Topic 5: 0.019*\"health\" + 0.019*\"value\" + 0.018*\"impact\" + 0.015*\"benefit\" + 0.014*\"service\" + 0.014*\"people\" + 0.012*\"support\" + 0.012*\"need\" + 0.011*\"experience\" + 0.011*\"potential\"\n",
      "Topic 6: 0.036*\"language\" + 0.018*\"english\" + 0.017*\"school\" + 0.015*\"learning\" + 0.012*\"teacher\" + 0.011*\"education\" + 0.010*\"resource\" + 0.009*\"student\" + 0.009*\"teaching\" + 0.008*\"different\"\n",
      "Topic 7: 0.033*\"data\" + 0.018*\"group\" + 0.015*\"practice\" + 0.013*\"based\" + 0.012*\"creative\" + 0.010*\"understanding\" + 0.009*\"impact\" + 0.009*\"knowledge\" + 0.008*\"change\" + 0.008*\"academic\"\n",
      "Topic 8: 0.014*\"material\" + 0.014*\"text\" + 0.014*\"library\" + 0.011*\"literature\" + 0.010*\"literary\" + 0.009*\"book\" + 0.009*\"wale\" + 0.009*\"early\" + 0.008*\"database\" + 0.008*\"translation\"\n",
      "Topic 9: 0.061*\"heritage\" + 0.026*\"museum\" + 0.026*\"site\" + 0.017*\"visitor\" + 0.013*\"resource\" + 0.011*\"history\" + 0.011*\"public\" + 0.011*\"place\" + 0.010*\"past\" + 0.010*\"landscape\"\n",
      "Topic 10: 0.053*\"cultural\" + 0.014*\"impact\" + 0.012*\"international\" + 0.011*\"global\" + 0.011*\"artist\" + 0.010*\"conflict\" + 0.010*\"country\" + 0.010*\"context\" + 0.010*\"indigenous\" + 0.008*\"practice\"\n",
      "Topic 11: 0.039*\"history\" + 0.017*\"event\" + 0.016*\"theatre\" + 0.016*\"museum\" + 0.014*\"archive\" + 0.014*\"audience\" + 0.013*\"british\" + 0.013*\"experience\" + 0.011*\"people\" + 0.010*\"local\"\n",
      "Topic 12: 0.070*\"network\" + 0.029*\"workshop\" + 0.022*\"public\" + 0.017*\"event\" + 0.016*\"academic\" + 0.011*\"museum\" + 0.010*\"engagement\" + 0.009*\"professional\" + 0.009*\"audience\" + 0.008*\"website\"\n",
      "Topic 13: 0.021*\"design\" + 0.011*\"innovation\" + 0.010*\"collection\" + 0.009*\"university\" + 0.009*\"sector\" + 0.009*\"business\" + 0.009*\"development\" + 0.008*\"national\" + 0.008*\"material\" + 0.008*\"industry\"\n",
      "Topic 14: 0.019*\"policy\" + 0.017*\"medium\" + 0.014*\"public\" + 0.014*\"woman\" + 0.012*\"young_people\" + 0.012*\"activity\" + 0.010*\"network\" + 0.009*\"stakeholder\" + 0.009*\"impact\" + 0.008*\"organisation\"\n"
     ]
    }
   ],
   "source": [
    "# Create a topic-term matrix where the seed words have high probabilities\n",
    "num_topics = 20\n",
    "alpha = 10\n",
    "eta = 0.01\n",
    "iterations = 200\n",
    "passes = 50\n",
    "\n",
    "lda_model = LdaModel(dtm, num_topics=num_topics, id2word=dictionary, alpha=alpha, eta=eta, iterations=iterations, passes = passes)\n",
    "\n",
    "# Assign the seed words to the topics\n",
    "for topic_id, topic_words in enumerate(seed_topics):\n",
    "    topic_words_ids = [dictionary.token2id[word] for word in topic_words if word in dictionary.token2id]\n",
    "    lda_model.get_topic_terms(topic_id, len(topic_words_ids))\n",
    "    for word_id, weight in lda_model.get_topic_terms(topic_id, len(topic_words_ids)):\n",
    "        lda_model.state.get_lambda()[topic_id, word_id] = 25.0\n",
    "\n",
    "# Train the LDA model with the seeded topics\n",
    "lda_model.update(dtm)\n",
    "\n",
    "# Print the topics with seeded words\n",
    "for i in range(num_topics):\n",
    "    print(f\"Topic {i}: {lda_model.print_topic(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f339c435-ba0c-459a-93a0-ab9fa0390eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdistributed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mupdate_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'symmetric'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgamma_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mminimum_probability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mns_conf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mper_word_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'numpy.float32'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Train and use Online Latent Dirichlet Allocation model as presented in\n",
       "`'Online Learning for LDA' by Hoffman et al.`_\n",
       "\n",
       "Examples\n",
       "-------\n",
       "Initialize a model using a Gensim corpus\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.test.utils import common_corpus\n",
       "    >>>\n",
       "    >>> lda = LdaModel(common_corpus, num_topics=10)\n",
       "\n",
       "You can then infer topic distributions on new, unseen documents.\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> doc_bow = [(1, 0.3), (2, 0.1), (0, 0.09)]\n",
       "    >>> doc_lda = lda[doc_bow]\n",
       "\n",
       "The model can be updated (trained) with new documents.\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> # In practice (corpus =/= initial training corpus), but we use the same here for simplicity.\n",
       "    >>> other_corpus = common_corpus\n",
       "    >>>\n",
       "    >>> lda.update(other_corpus)\n",
       "\n",
       "Model persistency is achieved through :meth:`~gensim.models.ldamodel.LdaModel.load` and\n",
       ":meth:`~gensim.models.ldamodel.LdaModel.save` methods.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Parameters\n",
       "----------\n",
       "corpus : iterable of list of (int, float), optional\n",
       "    Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\n",
       "    If you have a CSC in-memory matrix, you can convert it to a\n",
       "    streamed corpus with the help of gensim.matutils.Sparse2Corpus.\n",
       "    If not given, the model is left untrained (presumably because you want to call\n",
       "    :meth:`~gensim.models.ldamodel.LdaModel.update` manually).\n",
       "num_topics : int, optional\n",
       "    The number of requested latent topics to be extracted from the training corpus.\n",
       "id2word : {dict of (int, str), :class:`gensim.corpora.dictionary.Dictionary`}\n",
       "    Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n",
       "    debugging and topic printing.\n",
       "distributed : bool, optional\n",
       "    Whether distributed computing should be used to accelerate training.\n",
       "chunksize :  int, optional\n",
       "    Number of documents to be used in each training chunk.\n",
       "passes : int, optional\n",
       "    Number of passes through the corpus during training.\n",
       "update_every : int, optional\n",
       "    Number of documents to be iterated through for each update.\n",
       "    Set to 0 for batch learning, > 1 for online iterative learning.\n",
       "alpha : {float, numpy.ndarray of float, list of float, str}, optional\n",
       "    A-priori belief on document-topic distribution, this can be:\n",
       "        * scalar for a symmetric prior over document-topic distribution,\n",
       "        * 1D array of length equal to num_topics to denote an asymmetric user defined prior for each topic.\n",
       "\n",
       "    Alternatively default prior selecting strategies can be employed by supplying a string:\n",
       "        * 'symmetric': (default) Uses a fixed symmetric prior of `1.0 / num_topics`,\n",
       "        * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / (topic_index + sqrt(num_topics))`,\n",
       "        * 'auto': Learns an asymmetric prior from the corpus (not available if `distributed==True`).\n",
       "eta : {float, numpy.ndarray of float, list of float, str}, optional\n",
       "    A-priori belief on topic-word distribution, this can be:\n",
       "        * scalar for a symmetric prior over topic-word distribution,\n",
       "        * 1D array of length equal to num_words to denote an asymmetric user defined prior for each word,\n",
       "        * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination.\n",
       "\n",
       "    Alternatively default prior selecting strategies can be employed by supplying a string:\n",
       "        * 'symmetric': (default) Uses a fixed symmetric prior of `1.0 / num_topics`,\n",
       "        * 'auto': Learns an asymmetric prior from the corpus.\n",
       "decay : float, optional\n",
       "    A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
       "    when each new document is examined.\n",
       "    Corresponds to :math:`\\kappa` from `'Online Learning for LDA' by Hoffman et al.`_\n",
       "offset : float, optional\n",
       "    Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
       "    Corresponds to :math:`\\tau_0` from `'Online Learning for LDA' by Hoffman et al.`_\n",
       "eval_every : int, optional\n",
       "    Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
       "iterations : int, optional\n",
       "    Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
       "gamma_threshold : float, optional\n",
       "    Minimum change in the value of the gamma parameters to continue iterating.\n",
       "minimum_probability : float, optional\n",
       "    Topics with a probability lower than this threshold will be filtered out.\n",
       "random_state : {np.random.RandomState, int}, optional\n",
       "    Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
       "ns_conf : dict of (str, object), optional\n",
       "    Key word parameters propagated to :func:`gensim.utils.getNS` to get a Pyro4 nameserver.\n",
       "    Only used if `distributed` is set to True.\n",
       "minimum_phi_value : float, optional\n",
       "    if `per_word_topics` is True, this represents a lower bound on the term probabilities.\n",
       "per_word_topics : bool\n",
       "    If True, the model also computes a list of topics, sorted in descending order of most likely topics for\n",
       "    each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
       "callbacks : list of :class:`~gensim.models.callbacks.Callback`\n",
       "    Metric callbacks to log and visualize evaluation metrics of the model during training.\n",
       "dtype : {numpy.float16, numpy.float32, numpy.float64}, optional\n",
       "    Data-type to use during calculations inside model. All inputs are also converted.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\kusli\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\gensim\\models\\ldamodel.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     LdaMulticore, AuthorTopicModel"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LdaModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd49754-e01f-4ae1-9668-47e927a97085",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'guidedlda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mguidedlda\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'guidedlda'"
     ]
    }
   ],
   "source": [
    "import guidedlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb07cce-ea92-4f16-bbe6-e7c554724cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
